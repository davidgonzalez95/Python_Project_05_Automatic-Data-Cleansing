{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Creating Automated Data Cleaning Pipelines Using Python and Pandas\n", "## Table of Contents  \n", "1. [Project Description](#project-description)  \n", "2. [Standardize Your Data Import Process](#standardize-your-data-import-process)  \n", "3. [Implement Automated Data Validation](#implement-automated-data-validation)  \n", "4. [Create a Data Cleaning Pipeline](#create-a-data-cleaning-pipeline)    \n", "5. [Automate String Cleaning and Standardization](#automate-string-cleaning-and-standardization)  \n", "6. [Monitor Data Quality Over Time](#monitor-data-quality-over-time)  \n", "7. [Conclusion](#conclusion)  \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "from pathlib import Path\n", "\n", "def load_dataset(file_path, **kwargs):\n", "    \"\"\"\n", "    Load data from various file formats while handling common issues.\n", "    \"\"\"\n", "    file_type = Path(file_path).suffix.lower()\n", "    handlers = {\n", "        '.csv': pd.read_csv,\n", "        '.xlsx': pd.read_excel,\n", "        '.json': pd.read_json,\n", "        '.parquet': pd.read_parquet\n", "    }\n", "    reader = handlers.get(file_type)\n", "    if reader is None:\n", "        raise ValueError(f\"Unsupported file type: {file_type}\")\n", "    df = reader(file_path, **kwargs)\n", "    df.columns = df.columns.str.strip().str.lower()\n", "    df = df.replace('', pd.NA)\n", "    return df"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def validate_dataset(df, validation_rules=None):\n", "    \"\"\"\n", "    Apply validation rules to a dataframe and return validation results.\n", "    \"\"\"\n", "    if validation_rules is None:\n", "        validation_rules = {\n", "            'numeric_columns': {\n", "                'check_type': 'numeric',\n", "                'min_value': 0,\n", "                'max_value': 1000000\n", "            }\n", "        }\n", "    validation_results = {}\n", "    for column, rules in validation_rules.items():\n", "        if column not in df.columns:\n", "            continue\n", "        issues = []\n", "        missing_count = df[column].isna().sum()\n", "        if missing_count > 0:\n", "            issues.append(f\"Found {missing_count} missing values\")\n", "        validation_results[column] = issues\n", "    return validation_results"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DataCleaningPipeline:\n", "    def __init__(self):\n", "        self.steps = []\n", "    \n", "    def add_step(self, name, function):\n", "        self.steps.append({'name': name, 'function': function})\n", "    \n", "    def execute(self, df):\n", "        results = []\n", "        current_df = df.copy()\n", "        for step in self.steps:\n", "            try:\n", "                current_df = step['function'](current_df)\n", "                results.append({'step': step['name'], 'status': 'success'})\n", "            except Exception as e:\n", "                results.append({'step': step['name'], 'status': 'failed', 'error': str(e)})\n", "                break\n", "        return current_df, results"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def clean_text_columns(df, columns=None):\n", "    \"\"\"\n", "    Apply standardized text cleaning to specified columns.\n", "    \"\"\"\n", "    if columns is None:\n", "        columns = df.select_dtypes(include=['object']).columns\n", "    df = df.copy()\n", "    for column in columns:\n", "        if column not in df.columns:\n", "            continue\n", "        df[column] = (df[column]\n", "                     .astype(str)\n", "                     .str.strip()\n", "                     .str.lower()\n", "                     .replace(r'\\s+', ' ', regex=True)\n", "                     .replace(r'[^\\w\\s]', '', regex=True))\n", "    return df"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def generate_quality_metrics(df, baseline_metrics=None):\n", "    \"\"\"\n", "    Generate quality metrics for a dataset and compare with baseline if provided.\n", "    \"\"\"\n", "    metrics = {\n", "        'row_count': len(df),\n", "        'missing_values': df.isna().sum().to_dict(),\n", "        'unique_values': df.nunique().to_dict(),\n", "        'data_types': df.dtypes.astype(str).to_dict()\n", "    }\n", "    if baseline_metrics:\n", "        metrics['changes'] = {\n", "            'row_count_change': metrics['row_count'] - baseline_metrics['row_count']\n", "        }\n", "    return metrics"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}}, "nbformat": 4, "nbformat_minor": 4}