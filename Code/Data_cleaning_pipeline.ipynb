{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Automated Data Cleaning Pipelines Using Python and Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize Your Data Import Process\n",
    "\n",
    "def load_dataset(file_path, **kwargs):\n",
    "    \"\"\"\n",
    "    Load data from various file formats while handling common issues.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the data file\n",
    "        **kwargs: Additional arguments to pass to the appropriate pandas reader\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded and initially processed dataframe\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    \n",
    "    file_type = Path(file_path).suffix.lower()\n",
    "    \n",
    "    # Dictionary of file handlers\n",
    "    handlers = {\n",
    "        '.csv': pd.read_csv,\n",
    "        '.xlsx': pd.read_excel,\n",
    "        '.json': pd.read_json,\n",
    "        '.parquet': pd.read_parquet\n",
    "    }\n",
    "    \n",
    "    # Get appropriate reader function\n",
    "    reader = handlers.get(file_type)\n",
    "    if reader is None:\n",
    "        raise ValueError(f\"Unsupported file type: {file_type}\")\n",
    "    \n",
    "    # Load data with common cleaning parameters\n",
    "    df = reader(file_path, **kwargs)\n",
    "    \n",
    "    # Initial cleaning steps\n",
    "    df.columns = df.columns.str.strip().str.lower()  # Standardize column names\n",
    "    df = df.replace('', pd.NA)  # Convert empty strings to NA\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Automated Data Validation\n",
    "\n",
    "def validate_dataset(df, validation_rules=None):\n",
    "    \"\"\"\n",
    "    Apply validation rules to a dataframe and return validation results.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        validation_rules (dict): Dictionary of column names and their validation rules\n",
    "        \n",
    "    Returns:\n",
    "        dict: Validation results with issues found\n",
    "    \"\"\"\n",
    "    if validation_rules is None:\n",
    "        validation_rules = {\n",
    "            'numeric_columns': {\n",
    "                'check_type': 'numeric',\n",
    "                'min_value': 0,\n",
    "                'max_value': 1000000\n",
    "            },\n",
    "            'date_columns': {\n",
    "                'check_type': 'date',\n",
    "                'min_date': '2000-01-01',\n",
    "                'max_date': '2025-12-31'\n",
    "            }\n",
    "        }\n",
    "\n",
    "    validation_results = {}\n",
    "    \n",
    "    for column, rules in validation_rules.items():\n",
    "        if column not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        issues = []\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_count = df[column].isna().sum()\n",
    "        if missing_count > 0:\n",
    "            issues.append(f\"Found {missing_count} missing values\")\n",
    "            \n",
    "        # Type-specific validations\n",
    "        if rules['check_type'] == 'numeric':\n",
    "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "                issues.append(\"Column should be numeric\")\n",
    "            else:\n",
    "                out_of_range = df[\n",
    "                    (df[column] < rules['min_value']) | \n",
    "                    (df[column] > rules['max_value'])\n",
    "                ]\n",
    "                if len(out_of_range) > 0:\n",
    "                    issues.append(f\"Found {len(out_of_range)} values outside allowed range\")\n",
    "                    \n",
    "        validation_results[column] = issues\n",
    "    \n",
    "    return validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Data Cleaning Pipeline\n",
    "\n",
    "class DataCleaningPipeline:\n",
    "    \"\"\"\n",
    "    A modular pipeline for cleaning data with customizable steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.steps = []\n",
    "        \n",
    "    def add_step(self, name, function):\n",
    "        \"\"\"Add a cleaning step.\"\"\"\n",
    "        self.steps.append({'name': name, 'function': function})\n",
    "        \n",
    "    def execute(self, df):\n",
    "        \"\"\"Execute all cleaning steps in order.\"\"\"\n",
    "        results = []\n",
    "        current_df = df.copy()\n",
    "        \n",
    "        for step in self.steps:\n",
    "            try:\n",
    "                current_df = step['function'](current_df)\n",
    "                results.append({\n",
    "                    'step': step['name'],\n",
    "                    'status': 'success',\n",
    "                    'rows_affected': len(current_df)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    'step': step['name'],\n",
    "                    'status': 'failed',\n",
    "                    'error': str(e)\n",
    "                })\n",
    "                break\n",
    "                \n",
    "        return current_df, results\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "def standardize_dates(df):\n",
    "    date_columns = df.select_dtypes(include=['datetime64']).columns\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "pipeline = DataCleaningPipeline()\n",
    "pipeline.add_step('remove_duplicates', remove_duplicates)\n",
    "pipeline.add_step('standardize_dates', standardize_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automate String Cleaning and Standardization\n",
    "\n",
    "def clean_text_columns(df, columns=None):\n",
    "    \"\"\"\n",
    "    Apply standardized text cleaning to specified columns.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        columns (list): List of columns to clean. If None, clean all object columns\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with cleaned text columns\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=['object']).columns\n",
    "        \n",
    "    df = df.copy()\n",
    "    \n",
    "    for column in columns:\n",
    "        if column not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Apply string cleaning operations\n",
    "        df[column] = (df[column]\n",
    "                     .astype(str)\n",
    "                     .str.strip()\n",
    "                     .str.lower()\n",
    "                     .replace(r'\\s+', ' ', regex=True)  # Replace multiple spaces\n",
    "                     .replace(r'[^\\w\\s]', '', regex=True))  # Remove special characters\n",
    "                     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor Data Quality Over Time\n",
    "\n",
    "def generate_quality_metrics(df, baseline_metrics=None):\n",
    "    \"\"\"\n",
    "    Generate quality metrics for a dataset and compare with baseline if provided.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        baseline_metrics (dict): Previous metrics to compare against\n",
    "        \n",
    "    Returns:\n",
    "        dict: Current metrics and comparison with baseline\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'row_count': len(df),\n",
    "        'missing_values': df.isna().sum().to_dict(),\n",
    "        'unique_values': df.nunique().to_dict(),\n",
    "        'data_types': df.dtypes.astype(str).to_dict()\n",
    "    }\n",
    "    \n",
    "    # Add descriptive statistics for numeric columns\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "    metrics['numeric_stats'] = df[numeric_columns].describe().to_dict()\n",
    "    \n",
    "    # Compare with baseline if provided\n",
    "    if baseline_metrics:\n",
    "        metrics['changes'] = {\n",
    "            'row_count_change': metrics['row_count'] - baseline_metrics['row_count'],\n",
    "            'missing_values_change': {\n",
    "                col: metrics['missing_values'][col] - baseline_metrics['missing_values'][col]\n",
    "                for col in metrics['missing_values']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
